# 自然语言处理: 第二十五章 LLM 预训练监督微调阶段 tricks

**本片文章属于转载：**

原文地址: [LLM 预训练监督微调阶段 tricks - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/686642922)

<br />

<br />

<br />


## 一. LLM的预训练tricks

1. 使用“base”模型进行继续预训练（而不是“chat”模型），“base”模型的能力是最好的，因为还没有进行人类价值观的对齐训练。
2. 缩放定律（Scaling law）是非常重要的，Scaling law不仅适用于LLM的预训练，也适用于有监督微调阶段的训练。
3. 对于预训练过程的超参数设置（Batch size / 学习率）应当参考来自其他论文或者先前工作的一些设置。
4. 继续预训练需要百万，千万甚至上亿的tokens训练语料，过少的训练语料不支持进行预训练工作。
5. 多阶段的继续预训练可能是非常有用的，具体可参考Ziya2的工作。
6. 词表扩充通常是不必要的，除非你的语种真的属于小语种或者语种发生了与原tokenizer模型发生了根本性改变。

<br />

<br />

## 二. LLM的监督微调tricks

1. 数据质量非常重要，GPT-4是一个很好的数据标注工具。制造数据应当有现实的数据支持，全部的生成数据将促使“幻觉”问题的出现。
2. 如果你的微调数据很小（例如几百条），可以使用“chat”模型进行监督微调，较大的微调数据可以使用“base”模型进行监督微调。
3. 为了保持模型的一些原有能力，在监督微调阶段仍然需要融入一些通用的监督数据，而且通常情况下，通用监督数据数量是多于领域监督数据的（例如在训练医疗LLM时需要混合比医疗数据更多的通用监督数据）。
4. QLoRA有时候表现比LoRA的微调效果表现更好。
5. 监督微调过程通常不是为了给LLM注入更多知识，而是激活模型的对话能力，学习新知识通常在预训练阶段完成。如果不得不在监督微调阶段注入新知识，那么对于一条知识，则需要对应几十条不同的监督数据构造才能注入到LLM这条知识。
6. 监督微调阶段没有最好的参数设置，这个过程需要多次的实验设置最好的参数。


<br />


<br />


<br />


更多的实验 tricks 请参考CareGPT：GitHub - WangRongsheng/CareGPT: CareGPT (关怀GPT)是一个医疗大语言模型，同时它集合了数十个公开可用的医疗微调数据集和开放可用的医疗大语言模型，包含LLM的训练、测评、部署等以促进医疗LLM快速发展。Medical LLM, Open Source Driven for a Healthy Future.
